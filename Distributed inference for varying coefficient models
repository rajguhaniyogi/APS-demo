rm(list=ls())
## Libraries
library(mcmc)
library(MASS)
library(KernSmooth)
library(fields)
library(pscl)
library(mvtnorm)
library(spBayes)
library(MCMCpack)
library(Mposterior)
library(parallel)
library(doParallel)
library(foreach)
library(mgcv)

## Section 1: Simulate spatial data
set.seed(1)
samp.size <- 3000
train.size <- 2000
x.co <- runif(samp.size, 0, 1)
y.co <- runif(samp.size, 0, 1)
D <- as.matrix(dist(cbind(x.co, y.co)))
phi <- 2
sigmasq <- 1
tausq <- 0.1
mu <- 0.5
mat <- (sigmasq*exp(-phi*D))
w <- rmvn(1, rep(0, samp.size), mat)
Y <- rmvn(1, rep(mu, samp.size) + w, tausq*diag(samp.size))
## Training Data
dat.nomiss <- cbind(x.co[1:train.size],y.co[1:train.size],c(Y)[1:train.size]) 
## Test Data 
dat.miss <- cbind(x.co[(train.size+1):samp.size],y.co[(train.size+1):samp.size],c(Y)[(train.size+1):samp.size])

#### Section 2: Implement the distributed spatial inference
## Useful Quantities
## Total number of training samples
n.sample <- nrow(dat.nomiss)
## Total number of test samples 
n.test <- nrow(dat.miss)  
## Number of cores or subsets
n.core <- 10  
per.core <- floor(nrow(dat.nomiss)/n.core) 
## Number of observations in different subsets
n.part <- c(rep(per.core,n.core-1),n.sample-per.core*(n.core-1))  
## This is same as n.core
n.split <- length(n.part)  

## GP regression on data subsets

a <- 1:n.sample 
sample.loc <- dat.nomiss[,1:2] 
y <- dat.nomiss[,3]
## Training response
Y.train <- y  
## Training coordinates
coords.train <- sample.loc
## Test coordinates  
coords.pred <- dat.miss[,1:2] 
## Test sample size 
n.test <- nrow(dat.miss) 
index.part <- list() 
X.part <- list()
Y.part <- list()
coords.part <- list()

for(i in 1:n.split){
  beg<-Sys.time()
  index.part[[i]] <- sample(a,n.part[i],replace=FALSE) 
  ## Response in i th subset 
  Y.part[[i]] <- Y.train[index.part[[i]]]  
  ## Predictor in i th subset 
  X.part[[i]] <- rep(1,n.part[i])    
  ## Coordinates in i th subset       
  coords.part[[i]] <- coords.train[index.part[[i]],]  
  a <- setdiff(a,index.part[[i]])
}
## Divide predicted data and predict them
## independently in each subset
 p.sub <- c(0,seq(200,n.test,by=200)) 

##########################################
##Partitioned GP function
##Works with subset i, for i=1,...,n.core
##########################################

partitioned_GP <- function(i){
  ## Model fitting

  library(spBayes)
  starting <- list("phi"=3, "sigma.sq"=5, "tau.sq"=1) 
  tuning <- list("phi"=0.01, "sigma.sq"=0.01, "tau.sq"=0.01)
  priors.1 <- list("beta.Norm"=list(0, 1000),
                   "phi.Unif"=c(3/10, 3/0.1), "sigma.sq.IG"=c(2, 2),
                   "tau.sq.IG"=c(2, 0.1))
  cov.model <- "exponential"
  ## Number of MCMC iterations
  mcmc.sample <- 2000 
  ## Burn in 
  n.burn <- 0.5*mcmc.sample  
  ## Response in subset i
  ZZ <- Y.part[[i]]   
  ## Predictor in subset i    
  XX <- X.part[[i]] 
  ## Coordinates in subset i      
  CC <- coords.part[[i]]  

  ## GP computation in each subset
  m.1 <- spLM(ZZ~XX-1, coords=CC, starting=starting,
              tuning=tuning, priors=priors.1, cov.model=cov.model,
              n.samples=mcmc.sample,verbose=FALSE)           
  ## Recover all MCMC samples in each subset
  m.1.samp <- spRecover(m.1, start=n.burn+1, verbose=FALSE)  
  ## List of MCMC iterates from subset i
  subAtom <- cbind(m.1.samp$p.beta.recover.samples,m.1.samp$p.theta.recover.samples) 
  ## Delete this quantity from the memory
  rm(m.1.samp)
  ## Garbage cleaning  
  gc()          

  ## Prediction
  Y.pred <- t(spPredict(m.1, pred.covars=as.matrix(rep(1,p.sub[2]-p.sub[1])),
                           pred.coords=coords.pred[(p.sub[1]+1):p.sub[2],],
                           start=0.5*mcmc.sample)$p.y.predictive.samples)   

  for(l in 2:(length(p.sub)-1)){
     m.1.pred <- spPredict(m.1, pred.covars=as.matrix(rep(1,p.sub[l+1]-p.sub[l])),
                           pred.coords=coords.pred[(p.sub[l]+1):p.sub[l+1],],
                           start=0.5*mcmc.sample)$p.y.predictive.samples
     Y.pred <- cbind(Y.pred,t(m.1.pred))
  }
  ## MCMC samples for both parameter estimates and prediction after burn-in
  hh <- list(subAtom,Y.pred)  
  names(hh) <- c("atoms","predictions") 
  return(hh)
}

####### Parallelization ########
## Number of clusters for parallel implementation
cl<-makeCluster(n.core)  
registerDoParallel(cl)

## Start time
strt<-Sys.time()
## Parallelized subset computation of GP in different cores
obj <- foreach(i=1:n.core) %dopar% partitioned_GP(i)  
## Total time for parallelized inference
final.time <- Sys.time()-strt  
stopCluster(cl)


####### Combine ##################

subAtomList <- list()
for(i in 1:length(n.part)){
  ## MCMC samples to run Weiszfeld algorithm
  subAtomList[[i]] <- obj[[i]]$atoms
}
low.quant <- 0.025
mid.quant <- 0.5
upp.quant <- 0.975
quant.low.atoms <- matrix(NA,length(n.part),ncol(subAtomList[[1]]))
quant.med.atoms <- matrix(NA,length(n.part),ncol(subAtomList[[1]]))
quant.upp.atoms <- matrix(NA,length(n.part),ncol(subAtomList[[1]]))
for(i in 1:length(n.part)){
  quant.low.atoms[i,] <- apply(subAtomList[[i]],2,quantile,low.quant)
  quant.med.atoms[i,] <- apply(subAtomList[[i]],2,quantile,mid.quant)
  quant.upp.atoms[i,] <- apply(subAtomList[[i]],2,quantile,upp.quant)
}  
low.quant.combined <- colMeans(quant.low.atoms) ## 2.5% quantile
med.quant.combined <- colMeans(quant.med.atoms) ## median
upp.quant.combined <- colMeans(quant.upp.atoms) ## 97.5% quantile

####### Prediction ################ 

quant.low.atoms <- matrix(NA,length(n.part),ncol(obj[[1]]$predictions))
quant.med.atoms <- matrix(NA,length(n.part),ncol(obj[[1]]$predictions))
quant.upp.atoms <- matrix(NA,length(n.part),ncol(obj[[1]]$predictions))
for(i in 1:length(n.part)){
  quant.low.atoms[i,] <- apply(obj[[i]]$predictions,2,quantile,low.quant)
  quant.med.atoms[i,] <- apply(obj[[i]]$predictions,2,quantile,mid.quant)
  quant.upp.atoms[i,] <- apply(obj[[i]]$predictions,2,quantile,upp.quant)
}  
Y.lower_qnt <- colMeans(quant.low.atoms) ## 2.5% quantile
Y.med       <- colMeans(quant.med.atoms) ## median
Y.upper_qnt <- colMeans(quant.upp.atoms) ## 97.5% quantile
